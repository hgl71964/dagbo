import ax
import unittest
import time
from time import sleep

from dagbo.interface.exec_spark import *
from dagbo.interface.metrics_extractor import request_history_server, extract_app_id


class test_exec_spark(unittest.TestCase):
    def setUp(self):
        pass

    def tearDown(self):
        pass

    @unittest.skip("ok")
    def test_subprocess(self):

        rc = subprocess.run(["ls", "-l"])
        print(rc)
        print("...")

        t = time.time()
        rc = subprocess.run(["sleep", "3"])
        print(rc)
        print(time.time() - t)

        if rc.returncode != 0:
            print("return != 0")

    @unittest.skip("ok, this will overwrite conf file")
    def test_call_spark(self):
        conf_path = "/home/gh512/workspace/bo/spark-dir/hiBench/conf/spark.conf"
        exec_path = "/home/gh512/workspace/bo/spark-dir/hiBench/bin/workloads/micro/wordcount/spark/run.sh"

        # spec as generated by bo
        spec = {
            "executor.num[*]": 2,
            "executor.cores": 1,
            "executor.memory": 2,
            "shuffle.compress": 0,
        }
        call_spark(spec, conf_path, exec_path)

    def test_call_spark_with_feat_extraction(self):
        conf_path = "/home/gh512/workspace/bo/spark-dir/hiBench/conf/spark.conf"
        exec_path = "/home/gh512/workspace/bo/spark-dir/hiBench/bin/workloads/micro/wordcount/spark/run.sh"
        log_path = "/home/gh512/workspace/bo/spark-dir/hiBench/report/wordcount/spark/bench.log"
        base_url = "http://localhost:18080"

        # spec as generated by bo
        spec = {
            "executor.num[*]": 2,
            "executor.cores": 1,
            "executor.memory": 2,
            "shuffle.compress": 0,
        }
        # exec
        call_spark(spec, conf_path, exec_path)

        # extract
        sleep(5)  # give time to generate json in history server
        app_id = extract_app_id(log_path)
        metric = request_history_server(base_url, app_id)

        print(metric)


if __name__ == "__main__":
    unittest.main()
