import ax
import unittest
import time
from time import sleep

from dagbo.interface.exec_spark import *
from dagbo.utils.perf_model_utils import *
from dagbo.interface.metrics_extractor import *
from dagbo.interface.parse_performance_model import parse_model


class exec_spark_test(unittest.TestCase):
    def setUp(self):
        pass

    def tearDown(self):
        pass

    @unittest.skip("ok")
    def test_subprocess(self):

        rc = subprocess.run(["ls", "-l"])
        print(rc)
        print("...")

        t = time.time()
        rc = subprocess.run(["sleep", "3"])
        print(rc)
        print(time.time() - t)

        if rc.returncode != 0:
            print("return != 0")

    @unittest.skip("ok, this will overwrite conf file")
    def test_call_spark(self):
        conf_path = "/home/gh512/workspace/bo/spark-dir/hiBench/conf/spark.conf"
        exec_path = "/home/gh512/workspace/bo/spark-dir/hiBench/bin/workloads/micro/wordcount/spark/run.sh"

        # spec as generated by bo
        spec = {
            "executor.num[*]": 2,
            "executor.cores": 1,
            "executor.memory": 2,
            "shuffle.compress": 0,
        }
        call_spark(spec, conf_path, exec_path)

    @unittest.skip("ok")
    def test_call_spark_with_feat_extraction(self):
        conf_path = "/home/gh512/workspace/bo/spark-dir/hiBench/conf/spark.conf"
        exec_path = "/home/gh512/workspace/bo/spark-dir/hiBench/bin/workloads/micro/wordcount/spark/run.sh"
        log_path = "/home/gh512/workspace/bo/spark-dir/hiBench/report/wordcount/spark/bench.log"
        base_url = "http://localhost:18080"

        # spec as generated by bo
        spec = {
            "executor.num[*]": 6,  # can be over-subscribed
            "executor.cores": 4.1,  # can be over-subscribed
            "executor.memory": 2.2,
            "shuffle.compress": 0,
        }

        # exec
        call_spark(spec, conf_path, exec_path)

        # extract
        app_id = extract_app_id(log_path)
        metric = request_history_server(base_url, app_id)

        print(metric)

class perf_utils_test(unittest.TestCase):
    def setUp(self):
        param_space, metric_space, obj_space, edges = parse_model(
            "dagbo/interface/rosenbrock_20d_bo.txt")
            #"dagbo/interface/spark_performance_model.txt")

        self.param_space = param_space
        self.metric_space = metric_space
        self.obj_space = obj_space
        self.edges = edges
        #print(param_space)
        #print(edges)

    @unittest.skip("ok")
    def test_reversed_edge(self):
        reversed_edge = find_inverse_edges(self.edges)
        print(reversed_edge)

    #@unittest.skip("ok")
    def test_topological_sort(self):
        order = get_dag_topological_order(self.obj_space, self.edges)
        print(order)
        """
        A topological sort of a dag G = (V,E) is a linear ordering of all its vertices such that if G contains an edge (u,v),
            then u appears before v in the ordering.
        """
        for key in self.edges:
            for val in self.edges[key]:
                for node in order:
                    if key == node:  # find key first, ok
                        break
                    elif val == node:
                        raise RuntimeError("not topological order")


class perf_model_test(unittest.TestCase):
    def setUp(self):
        # performance model
        param_space, metric_space, obj_space, edges = parse_model(
            "dagbo/interface/rosenbrock_20d_bo.txt")
            #"dagbo/interface/rosenbrock_3d_bo.txt")
            #"dagbo/interface/rosenbrock_3d_correct_model.txt")
            #"dagbo/interface/spark_performance_model.txt")

        self.param_space = param_space
        self.metric_space = metric_space
        self.obj_space = obj_space
        self.edges = edges
        #print(param_space)
        #print(edges)

        acq_func_config = {
            "q": 2,
            "num_restarts": 48,
            "raw_samples": 128,
            "num_samples": 2048,
            "y_max": torch.tensor([1.]),  # for EI
            "beta": 1,  # for UCB
        }
        self.acq_func_config = acq_func_config

        # make fake input tensor
        self.train_inputs_dict = {
            i: torch.rand(acq_func_config["q"])
            for i in list(param_space.keys())
        }
        self.train_targets_dict = {
            i: torch.rand(acq_func_config["q"])
            for i in list(metric_space.keys()) + list(obj_space.keys())
        }
        norm = True

        # build, build_perf_model_from_spec
        self.dag = build_perf_model_from_spec_ssa(self.train_inputs_dict,
                                              self.train_targets_dict,
                                              acq_func_config["num_samples"],
                                              param_space, metric_space,
                                              obj_space, edges, norm)
        # feature extractor
        self.app_id = "application_1641844906451_0006"
        self.base_url = "http://localhost:18080"

    #@unittest.skip("ok")
    def test_input_build(self):
        node_order = get_dag_topological_order(self.obj_space, self.edges)
        train_input_names, train_target_names, train_inputs, train_targets = build_input_by_topological_order(self.train_inputs_dict,
                                              self.train_targets_dict,
                                              self.param_space, self.metric_space,
                                              self.obj_space, node_order)
        print("input build:")
        print(train_input_names)
        print(train_target_names)
        print(train_inputs.shape)
        print(train_targets.shape)

    #@unittest.skip("ok")
    def test_dag_build(self):
        print(self.dag)

    @unittest.skip("ok")
    def test_feat_extract(self):
        metric = request_history_server(self.base_url, self.app_id)
        print(metric)

    @unittest.skip("ok")
    def test_extract_throughput(self):
        path = "/home/gh512/workspace/bo/spark-dir/hiBench/report/hibench.report"
        l = extract_throughput(path)
        print(l)

    @unittest.skip("ok")
    def test_app_id_extract(self):
        log_path = "/home/gh512/workspace/bo/spark-dir/hiBench/report/wordcount/spark/bench.log"
        app_id = extract_app_id(log_path)
        print("app id")
        print(app_id)

    @unittest.skip("ok")
    def test_app_id_feat_extraction(self):
        log_path = "/home/gh512/workspace/bo/spark-dir/hiBench/report/wordcount/spark/bench.log"
        app_id = extract_app_id(log_path)
        metric = request_history_server(self.base_url, app_id)

        print("end-to-end feat extraction: ")
        print(metric)


if __name__ == "__main__":
    unittest.main()
