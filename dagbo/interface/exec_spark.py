import os
import subprocess
from typing import Union

# example:
#rc = subprocess.call("benchmarks/sleep.sh", shell=True)
#print(rc)
#
#rc = subprocess.run(["ls", "-l"])
#print(rc)
#print("...")
#
#rc = subprocess.run(["ls", "-l", "/dev/null"], capture_output=True)
#print(rc)
"""
spawn a child process and execute spark job with given parameters
"""

# this will be written to spark.conf regardless input parameters
CONST_WRITE = {
    "hibench.spark.home": "/local/scratch/opt/spark-2.4.5-bin-hadoop2.7",
    "hibench.spark.master": "yarn",
    "spark.eventLog.enabled": "true",
}

# map from param to actual spark config name
NAME_MAPPING = {
    "executor.num[*]": "hibench.yarn.executor.num",
    "executor.cores": "hibench.yarn.executor.cores",
    "shuffle.compress": "spark.shuffle.compress",
    "executor.memory": "spark.executor.memory",
    "memory.fraction": "spark.memory.fraction",
    "spark.serializer": "spark.serializer",
    "rdd.compress": "spark.rdd.compress",
    "default.parallelism": "spark.default.parallelism",
    "shuffle.spill.compress": "spark.shuffle.spill.compress",
    "spark.speculation": "spark.speculation",
}

# spec that needs unit mapping, e.g. '4' -> '4g'
UNIT_MAPPING = {
    "spark.executor.memory": 0,
}

# 0 -> false
BOOL_MAPPING = {
    "spark.shuffle.compress": 0,
    "spark.rdd.compress": 0,
    "spark.shuffle.spill.compress": 0,
    "spark.speculation": 0,
}


def call_spark(param: dict[str, Union[float, int]], file_path: str,
               exec_path: str) -> None:
    """
    call hibench spark benchmarks with the given parameters
    Args:
        param: param file generated by bo
        file_path: the abs path to put the configuration file
        exec_path: the abs path to executable
    """

    # pre-processing param
    param_ = _pre_process(param)

    # write spec according to param
    _write_spec_from_param(param_, file_path)

    # exec
    _exec(exec_path)

    return


def _exec(exec_path: str) -> None:
    rc = subprocess.run([exec_path])
    if rc.returncode != 0:
        print("stderr: ")
        print(rc.stderr)
        raise RuntimeError("exec spark return non-zero")
    return None


def _write_spec_from_param(param: dict[str, str], file_path: str) -> None:
    # remove if exists
    if os.path.exists(file_path):
        os.remove(file_path)

    # write
    with open(file_path, "a") as f:
        for key, val in CONST_WRITE.items():
            f.write(key)
            f.write("\t")
            f.write(val)
            f.write("\n")

        for key, val in param.items():
            f.write(key)
            f.write("\t")
            f.write(val)
            f.write("\n")

    return None


def _pre_process(param: dict[str, Union[float, int]]) -> dict[str, str]:
    """
    perform type conversion?
    """
    param_ = {}

    # map from param to actual spark config name
    for key, val in param.items():
        param_[NAME_MAPPING[key]] = str(val)

    # perform unit mapping, e.g. '4' -> '4g'
    for key in list(param_.keys()):
        if key in UNIT_MAPPING:
            param_[key] = str(param_[key]) + "g"

    # perform bool mapping
    for key in list(param_.keys()):
        if key in BOOL_MAPPING:
            val = param_[key]
            if val == "0":
                param_[key] = "false"
            elif val == "1":
                param_[key] = "true"
            else:
                raise ValueError("unknown boolean val")

    # perform data type conversion? e.g. mapping float to int XXX
    return param_
