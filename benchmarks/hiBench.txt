BUILD:

as of hiBench version 29224b9c

build with java 8: sudo apt install openjdk-8-jdk

maven version: 3.2.5

apache spark version 2.4.5

apacha hadoop version 2.7.7

ENV VARIABLES(change accordingly):

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64  # use jvm 8 to work with hiBench
export M2_HOME=/local/scratch/opt/apache-maven-3.2.5
export MAVEN_HOME=/local/scratch/opt/apache-maven-3.2.5
export PATH=${M2_HOME}/bin:${PATH}

# spark & hadoop
export HADOOP_CONF_DIR=/local/scratch/opt/hadoop-2.7.7/etc/hadoop
export HADOOP_HOME=/local/scratch/opt/hadoop-2.7.7
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export SPARK_HOME=/local/scratch/opt/spark-2.4.5-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=/usr/bin/python3.7

CONFIG DIR:

1. {hadoop home}/etc/hadoop
        - core-site.xml, yarn-site.xml, hdfs-site.xml, yarn-site.xml
2. {hibench home}/conf


DISTRIBUTED ENV SET-UPS:

1. In /etc/hosts, list all machines hostname & ip for all machines

2. exchange ssh key with password between all machines, ensure any one can ssh into any other (by hostname & self)
        put pub key in ~/.ssh/authorized_keys
        (for google cloud compute, add key to vm's meta-data)

3. config hadoop file in {hadoop home}/etc/hadoop

        In core-site.xml
        '''
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://{master-hostname}:9000</value>
        </property>
        '''

        In yarn-site.xml
        '''
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>{master-hostname}</value>
        </property>
        <property>
                <name>yarn.nodemanager.hostname</name>
                <value>{master-hostname}</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>

        # other config related container
        <property> <!-- for this node -->
                <name>yarn.nodemanager.resource.memory-mb</name>
                <value>24000</value>
        </property>
        <property> <!-- for one container -->
                <name>yarn.scheduler.minimum-allocation-mb</name>
                <value>1024</value>
        </property>
        <property> <!-- for one container -->
                <name>yarn.scheduler.maximum-allocation-mb</name>
                <value>8192</value>
        </property>

        # to avoid running out of mem limited
        <property>
                <name>yarn.nodemanager.pmem-check-enabled</name>
                <value>false</value>
        </property>
        <property>
                <name>yarn.nodemanager.vmem-check-enabled</name>
                <value>false</value>
        </property>
        '''

        In mapred-site.xml
        '''
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        '''

        In hdfs-site.xml
        '''
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>{path to local fs dir}</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>{path to local fs dir}</value>
        </property>

        # to change to hostname of a datanode (as shown in hdfs dfsadmin -report)
	<property>
		  <name>dfs.datanode.hostname</name>
		  <value>IP-ADDRESS</value>
	</property>
        '''

        In masters
                list master hostname
        In slaves
                list slaves hostname

4. config Hibench conf
        In conf/hadoop.conf, ensure hostname correct etc..
        In conf/hibench.conf, ensure hostname correct etc..

If successfully set-up the hdfs cluster, we can do `hdfs dfsadmin -report` to see all nodes in our cluster
        NOTE: ensure each node shows their hostname, not `localhost`, can use dfs's config to enforce this?
If successfully set-up the YARN cluster, we can do `yarn node -list` to see all nodes in our cluster
        NOTE: ensure each node shows their hostname, not `localhost`, can specify in yarn-site's yarn.nodemanager.address


EXPERIMENT ON NEW CLUTERS:

1. ensure the above config is ok
2. in auto-experiment.sh, set up the corresponding params
3. in exec_spark.py, set up the corresponding scale mapping

NOTE:

1. when run with YARN, don't have to start-master.sh/start-slave.sh by ourselves
        only need to spark-submit with --master yarn

2. history-server log dir (default): /tmp/spark-events


FAILUER:

Spark Bench:

GENERALLY, rebuild Hibench & restart all services

0. unbound variable
install python2, make conf file in {hibench home}/conf

1. fail to launch because container run beyond virtual memory limit,

it could be a problem for java 8
this is found from log -> {hadoop home}/logs/yarn-gh512-resourcemanager

solutions:
        we have the minimum RAM for a Container (yarn.scheduler.minimum-allocation-mb) = 2 GB
        this is configurable in {hadoop home}/etc/hadoop/yarn-site.xml

        I simply set executor memory = 6g (2 executors) solve the problem
        also see: https://stackoverflow.com/questions/43826703/difference-between-yarn-scheduler-maximum-allocation-mb-and-yarn-nodemanager

2. fail to run jobs, as yarn scheduler `hang`
this means there is insufficient recourses to run any single jobs

solution:
        OFTEN: just restart yarn & hdfs

        if the above doesn't work:
        check logs at {hadoop home}/logs/yarn*
        for capacity issues, check: https://stackoverflow.com/questions/33465300/why-does-yarn-job-not-transition-to-running-state
        i.e. in capacity-scheduler.xml check: yarn.scheduler.capacity.maximum-am-resource-percent

3. fail with `no space left on device`
By default Spark uses the /tmp directory to store intermediate data

solution:
        spark.local.dir                     SOME/DIR/WHERE/YOU/HAVE/SPACE
        in ${hadoop/etc/core-site.xml}      set `hadoop.tmp.dir` as a property

4. fail with connection refused
either namenode OR datanode may not be started properly!

solution:
        stop-dfs.sh
        for datanote: delete rm -rf .../datanode/*, then start-dfs.sh
        for namenode: hadoop namenode -format

5. fail with connection reset by peer:
        https://stackoverflow.com/questions/39347392/how-to-fix-connection-reset-by-peer-message-from-apache-spark

6. fail with permission denied in bench.log
        rm bench.log per iteration?

7. fail to launch history server
       make sure /tmp/spark-events exists! & permission 777
